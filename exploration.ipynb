{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-01T12:26:07.915454Z",
     "start_time": "2024-03-01T12:26:05.636527Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# This must be first\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\".env\")\n",
    "\n",
    "import os\n",
    "import string\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from core.utils.misc import seed_everything\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)\r\n",
      "/usr/bin/python\r\n"
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-01T12:26:08.155649Z",
     "start_time": "2024-03-01T12:26:07.917177Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-01T12:27:00.109893Z",
     "start_time": "2024-03-01T12:26:08.156706Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Giving up using llama.. Using dummy path. If you want to use llama, please set the environment variable LLAMA_DIR to the path of the llama directory, and modify the llama_local_path function in core/models/llm_loading.py to return the correct path.\n",
      "Giving up using llama.. Using dummy path. If you want to use llama, please set the environment variable LLAMA_DIR to the path of the llama directory, and modify the llama_local_path function in core/models/llm_loading.py to return the correct path.\n",
      "Giving up using llama.. Using dummy path. If you want to use llama, please set the environment variable LLAMA_DIR to the path of the llama directory, and modify the llama_local_path function in core/models/llm_loading.py to return the correct path.\n",
      "Giving up using llama.. Using dummy path. If you want to use llama, please set the environment variable LLAMA_DIR to the path of the llama directory, and modify the llama_local_path function in core/models/llm_loading.py to return the correct path.\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1b9e1891af8d4d08a900c1ed73c5e4f4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from core.utils.misc import limit_gpus\n",
    "from core.models.llm_loading import load_model_and_tokenizer, load_tokenizer\n",
    "\n",
    "limit_gpus(range(0, 2))\n",
    "\n",
    "# model_type, model_variant = \"gpt-2\", \"1.5B\"\n",
    "model_type, model_variant = \"llama\", \"7B\"\n",
    "# model_type, model_variant = \"gpt-j\", \"6B\"\n",
    "# model_type, model_variant = \"pythia\", \"2.8B\"\n",
    "# model_type, model_variant = \"pythia\", \"6.9B\"\n",
    "# model_type, model_variant = \"falcon\", \"7B\"\n",
    "# model_type, model_variant = \"mpt\", \"7B\"\n",
    "\n",
    "model, tokenizer = load_model_and_tokenizer(model_type, model_variant)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In Context Learning Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-01T12:27:35.430779Z",
     "start_time": "2024-03-01T12:27:24.909585Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading wordnet: <urlopen error [Errno 101] Network\n",
      "[nltk_data]     is unreachable>\n",
      "[nltk_data] Error loading omw-1.4: <urlopen error [Errno 101] Network\n",
      "[nltk_data]     is unreachable>\n",
      "/home/meilingrui/anaconda3/envs/llm/lib/python3.11/site-packages/transformers/generation/utils.py:1413: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out:\t ['determine', 'the', 'already', 'demand', 'network', 'happy', 'second', 'once', 'step', 'domain', 'build', 'save', 'be', 'home', 'play', 'choose', 'avoid', 'date', 'person', 'the', 'candidate', 'proper', 'hold', 'century', 'set', 'claim', 'tree', 'believe', 'little', 'huge', 'probable', 'letter', 'decide', 'video', 'share', 'agency', 'action', 'minute', 'protect', 'face', 'quickly', 'a', 'course', 'first', 'question', 'resource', 'long', 'cell', 'local', 'identify', 'stranger', 'work', 'earth', 'can', 'prove', 'course', 'wall', 'the', 'specific', 'arm', 'sun', 'French', 'begin', 'need', 'argument', 'any', 'summer', 'make', 'best', 'test', 'hell', 'many', 'seem', 'miss', 'student', 'station', 'clearly', 'and', 'culture', 'dead', 'once', 'death', 'camera', 'apply', 'child', 'better', 'pay', 'realize', 'earth', 'pay', 'pass', 'lead', 'water', 'color', 'size', 'left', 'try', 'there', 'major', 'drug']\n",
      "Exp.:\t [' determine', ' window', ' already', ' require', ' network', ' happy', ' second', ' once', ' stage', ' field', ' build', ' save', ' be', ' home', ' play', ' choose', ' avoid', ' date', ' nobody', ' price', ' candidate', ' own', ' hold', ' century', ' whole', ' claim', ' tree', ' believe', ' few', ' huge', ' likely', ' letter', ' decide', ' video', ' share', ' agency', ' action', ' minute', ' protect', ' face', ' quickly', ' one', ' course', ' first', ' question', ' resource', ' long', ' cell', ' local', ' identify', ' foreign', ' job', ' earth', ' can', ' prove', ' course', ' wall', ' standard', ' certain', ' arm', ' sun', ' en', ' begin', ' need', ' argument', ' any', ' summer', ' manage', ' best', ' test', ' hell', ' several', ' seem', ' miss', ' student', ' station', ' clearly', ' purpose', ' culture', ' dead', ' once', ' death', ' camera', ' apply', ' kid', ' better', ' pay', ' realize', ' earth', ' pay', ' pass', ' lead', ' water', ' color', ' size', ' left', ' try', ' there', ' major', ' drug']\n",
      "Accuracy: 0.86\n",
      "Error cases:\n",
      "Input          Output         Expected       \n",
      " but           and             purpose       \n",
      " certain       specific        certain       \n",
      " ensemble      set             whole         \n",
      " fr            French          en            \n",
      " la fenêtre    the             window        \n",
      " la norme      the             standard      \n",
      " le prix       the             price         \n",
      " nombreuses    many            several       \n",
      " personne      person          nobody        \n",
      " peu           little          few           \n",
      " propre        proper          own           \n",
      " travail       work            job           \n",
      " un            a               one           \n",
      " étape         step            stage         \n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:1! (when checking argument for argument tensors in method wrapper_CUDA_cat)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 46\u001B[0m\n\u001B[1;32m     43\u001B[0m icl_predictions \u001B[38;5;241m=\u001B[39m run_icl(model, tokenizer, task, test_datasets)\n\u001B[1;32m     44\u001B[0m print_evaluation_summary(task, icl_predictions, test_datasets)\n\u001B[0;32m---> 46\u001B[0m tv_predictions, tv_dev_accuracy_by_layer, task_hiddens \u001B[38;5;241m=\u001B[39m run_task_vector(\n\u001B[1;32m     47\u001B[0m     model,\n\u001B[1;32m     48\u001B[0m     tokenizer,\n\u001B[1;32m     49\u001B[0m     task,\n\u001B[1;32m     50\u001B[0m     test_datasets,\n\u001B[1;32m     51\u001B[0m     dev_datasets,\n\u001B[1;32m     52\u001B[0m     multi_context\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m     53\u001B[0m )\n\u001B[1;32m     54\u001B[0m print_evaluation_summary(task, tv_predictions, test_datasets)\n",
      "File \u001B[0;32m~/workspace/icl_task_vectors/core/task_vectors.py:48\u001B[0m, in \u001B[0;36mrun_task_vector\u001B[0;34m(model, tokenizer, task, test_datasets, dev_datasets, layers_to_test, multi_context)\u001B[0m\n\u001B[1;32m     39\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrun_task_vector\u001B[39m(\n\u001B[1;32m     40\u001B[0m     model: PreTrainedModel,\n\u001B[1;32m     41\u001B[0m     tokenizer: PreTrainedTokenizer,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     46\u001B[0m     multi_context: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m     47\u001B[0m ):\n\u001B[0;32m---> 48\u001B[0m     dev_accuracy_by_layer \u001B[38;5;241m=\u001B[39m task_vector_accuracy_by_layer(\n\u001B[1;32m     49\u001B[0m         model,\n\u001B[1;32m     50\u001B[0m         tokenizer,\n\u001B[1;32m     51\u001B[0m         task,\n\u001B[1;32m     52\u001B[0m         dev_datasets,\n\u001B[1;32m     53\u001B[0m         layers_to_test\u001B[38;5;241m=\u001B[39mlayers_to_test,\n\u001B[1;32m     54\u001B[0m         multi_context\u001B[38;5;241m=\u001B[39mmulti_context,\n\u001B[1;32m     55\u001B[0m     )\n\u001B[1;32m     56\u001B[0m     best_intermediate_layer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mint\u001B[39m(\u001B[38;5;28mmax\u001B[39m(dev_accuracy_by_layer, key\u001B[38;5;241m=\u001B[39mdev_accuracy_by_layer\u001B[38;5;241m.\u001B[39mget))\n\u001B[1;32m     58\u001B[0m     task_hiddens \u001B[38;5;241m=\u001B[39m get_task_hiddens(model, tokenizer, task, test_datasets, multi_context\u001B[38;5;241m=\u001B[39mmulti_context)\n",
      "File \u001B[0;32m~/workspace/icl_task_vectors/core/task_vectors.py:265\u001B[0m, in \u001B[0;36mtask_vector_accuracy_by_layer\u001B[0;34m(model, tokenizer, task, datasets, layers_to_test, multi_context)\u001B[0m\n\u001B[1;32m    263\u001B[0m accuracies \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m    264\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m layer_num \u001B[38;5;129;01min\u001B[39;00m layers_to_test:\n\u001B[0;32m--> 265\u001B[0m     answers \u001B[38;5;241m=\u001B[39m modulated_generate(\n\u001B[1;32m    266\u001B[0m         model,\n\u001B[1;32m    267\u001B[0m         tokenizer,\n\u001B[1;32m    268\u001B[0m         task,\n\u001B[1;32m    269\u001B[0m         datasets,\n\u001B[1;32m    270\u001B[0m         intermediate_layer\u001B[38;5;241m=\u001B[39mlayer_num,\n\u001B[1;32m    271\u001B[0m         task_hiddens\u001B[38;5;241m=\u001B[39mtask_hiddens,\n\u001B[1;32m    272\u001B[0m         past_key_values\u001B[38;5;241m=\u001B[39mpast_key_values,\n\u001B[1;32m    273\u001B[0m     )\n\u001B[1;32m    275\u001B[0m     accuracy \u001B[38;5;241m=\u001B[39m calculate_accuracy_on_datasets(task, answers, datasets)\n\u001B[1;32m    276\u001B[0m     accuracies\u001B[38;5;241m.\u001B[39mappend(accuracy)\n",
      "File \u001B[0;32m~/workspace/icl_task_vectors/core/task_vectors.py:188\u001B[0m, in \u001B[0;36mmodulated_generate\u001B[0;34m(model, tokenizer, task, test_datasets, task_hiddens, intermediate_layer, past_key_values, return_task_hiddens, include_train)\u001B[0m\n\u001B[1;32m    175\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mmodulated_generate\u001B[39m(\n\u001B[1;32m    176\u001B[0m     model: PreTrainedModel,\n\u001B[1;32m    177\u001B[0m     tokenizer: PreTrainedTokenizer,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    184\u001B[0m     include_train: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    185\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m List[\u001B[38;5;28mstr\u001B[39m]:\n\u001B[1;32m    186\u001B[0m     inputs \u001B[38;5;241m=\u001B[39m tokenize_datasets(tokenizer, test_datasets, format_dataset_kwargs\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minclude_train\u001B[39m\u001B[38;5;124m\"\u001B[39m: include_train})\n\u001B[0;32m--> 188\u001B[0m     first_forward_outputs \u001B[38;5;241m=\u001B[39m modulated_forward(\n\u001B[1;32m    189\u001B[0m         model,\n\u001B[1;32m    190\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    191\u001B[0m         task_hiddens\u001B[38;5;241m=\u001B[39mtask_hiddens,\n\u001B[1;32m    192\u001B[0m         intermediate_layer\u001B[38;5;241m=\u001B[39mintermediate_layer,\n\u001B[1;32m    193\u001B[0m         past_key_values\u001B[38;5;241m=\u001B[39mpast_key_values,\n\u001B[1;32m    194\u001B[0m     )\n\u001B[1;32m    195\u001B[0m     first_predicted_token_ids \u001B[38;5;241m=\u001B[39m first_forward_outputs\u001B[38;5;241m.\u001B[39mlogits[:, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m.\u001B[39margmax(dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m    196\u001B[0m     answers \u001B[38;5;241m=\u001B[39m decode_predictions(first_predicted_token_ids, tokenizer)\n",
      "File \u001B[0;32m~/workspace/icl_task_vectors/core/task_vectors.py:229\u001B[0m, in \u001B[0;36mmodulated_forward\u001B[0;34m(model, inputs, task_hiddens, intermediate_layer, batch_size, past_key_values)\u001B[0m\n\u001B[1;32m    226\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m past_key_values \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    227\u001B[0m     inputs[get_input_type(inputs)] \u001B[38;5;241m=\u001B[39m inputs[get_input_type(inputs)][:, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m--> 229\u001B[0m first_forward_outputs \u001B[38;5;241m=\u001B[39m modified_forward(\n\u001B[1;32m    230\u001B[0m     model,\n\u001B[1;32m    231\u001B[0m     inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    232\u001B[0m     forward_kwargs\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpast_key_values\u001B[39m\u001B[38;5;124m\"\u001B[39m: past_key_values},\n\u001B[1;32m    233\u001B[0m     forward_modifiers\u001B[38;5;241m=\u001B[39mforward_modifiers,\n\u001B[1;32m    234\u001B[0m     batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mlen\u001B[39m(inputs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minput_ids\u001B[39m\u001B[38;5;124m\"\u001B[39m]),  \u001B[38;5;66;03m# TODO: need to enable batched forward with HiddenInjector\u001B[39;00m\n\u001B[1;32m    235\u001B[0m )\n\u001B[1;32m    237\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m first_forward_outputs\n",
      "File \u001B[0;32m~/workspace/icl_task_vectors/core/models/utils/inference.py:49\u001B[0m, in \u001B[0;36mmodified_forward\u001B[0;34m(model, inputs, forward_kwargs, batch_size, forward_modifiers)\u001B[0m\n\u001B[1;32m     47\u001B[0m context_manager \u001B[38;5;241m=\u001B[39m modified_forward_context_manager(model, forward_modifiers\u001B[38;5;241m=\u001B[39mforward_modifiers)\n\u001B[1;32m     48\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m context_manager:\n\u001B[0;32m---> 49\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m batch_forward(\n\u001B[1;32m     50\u001B[0m         model,\n\u001B[1;32m     51\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m     52\u001B[0m         forward_kwargs\u001B[38;5;241m=\u001B[39mforward_kwargs,\n\u001B[1;32m     53\u001B[0m         batch_size\u001B[38;5;241m=\u001B[39mbatch_size,\n\u001B[1;32m     54\u001B[0m     )\n\u001B[1;32m     56\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m outputs\n",
      "File \u001B[0;32m~/workspace/icl_task_vectors/core/models/utils/inference.py:110\u001B[0m, in \u001B[0;36mbatch_forward\u001B[0;34m(model, inputs, forward_kwargs, batch_size, show_progress)\u001B[0m\n\u001B[1;32m    107\u001B[0m batch_inputs \u001B[38;5;241m=\u001B[39m nested_apply(batch_inputs, \u001B[38;5;28;01mlambda\u001B[39;00m t: t\u001B[38;5;241m.\u001B[39mto(device))\n\u001B[1;32m    109\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[0;32m--> 110\u001B[0m     out \u001B[38;5;241m=\u001B[39m model(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mbatch_inputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mforward_kwargs)\n\u001B[1;32m    111\u001B[0m     output_class \u001B[38;5;241m=\u001B[39m out\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\n\u001B[1;32m    112\u001B[0m     out \u001B[38;5;241m=\u001B[39m nested_apply(out, \u001B[38;5;28;01mlambda\u001B[39;00m t: t\u001B[38;5;241m.\u001B[39mcpu())\n",
      "File \u001B[0;32m~/anaconda3/envs/llm/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/anaconda3/envs/llm/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/llm/lib/python3.11/site-packages/accelerate/hooks.py:166\u001B[0m, in \u001B[0;36madd_hook_to_module.<locals>.new_forward\u001B[0;34m(module, *args, **kwargs)\u001B[0m\n\u001B[1;32m    164\u001B[0m         output \u001B[38;5;241m=\u001B[39m module\u001B[38;5;241m.\u001B[39m_old_forward(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    165\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 166\u001B[0m     output \u001B[38;5;241m=\u001B[39m module\u001B[38;5;241m.\u001B[39m_old_forward(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    167\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m module\u001B[38;5;241m.\u001B[39m_hf_hook\u001B[38;5;241m.\u001B[39mpost_forward(module, output)\n",
      "File \u001B[0;32m~/anaconda3/envs/llm/lib/python3.11/site-packages/transformers/models/mistral/modeling_mistral.py:1154\u001B[0m, in \u001B[0;36mMistralForCausalLM.forward\u001B[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m   1151\u001B[0m return_dict \u001B[38;5;241m=\u001B[39m return_dict \u001B[38;5;28;01mif\u001B[39;00m return_dict \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39muse_return_dict\n\u001B[1;32m   1153\u001B[0m \u001B[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001B[39;00m\n\u001B[0;32m-> 1154\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel(\n\u001B[1;32m   1155\u001B[0m     input_ids\u001B[38;5;241m=\u001B[39minput_ids,\n\u001B[1;32m   1156\u001B[0m     attention_mask\u001B[38;5;241m=\u001B[39mattention_mask,\n\u001B[1;32m   1157\u001B[0m     position_ids\u001B[38;5;241m=\u001B[39mposition_ids,\n\u001B[1;32m   1158\u001B[0m     past_key_values\u001B[38;5;241m=\u001B[39mpast_key_values,\n\u001B[1;32m   1159\u001B[0m     inputs_embeds\u001B[38;5;241m=\u001B[39minputs_embeds,\n\u001B[1;32m   1160\u001B[0m     use_cache\u001B[38;5;241m=\u001B[39muse_cache,\n\u001B[1;32m   1161\u001B[0m     output_attentions\u001B[38;5;241m=\u001B[39moutput_attentions,\n\u001B[1;32m   1162\u001B[0m     output_hidden_states\u001B[38;5;241m=\u001B[39moutput_hidden_states,\n\u001B[1;32m   1163\u001B[0m     return_dict\u001B[38;5;241m=\u001B[39mreturn_dict,\n\u001B[1;32m   1164\u001B[0m )\n\u001B[1;32m   1166\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m   1167\u001B[0m logits \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlm_head(hidden_states)\n",
      "File \u001B[0;32m~/anaconda3/envs/llm/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/anaconda3/envs/llm/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/llm/lib/python3.11/site-packages/transformers/models/mistral/modeling_mistral.py:1039\u001B[0m, in \u001B[0;36mMistralModel.forward\u001B[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m   1029\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_gradient_checkpointing_func(\n\u001B[1;32m   1030\u001B[0m         decoder_layer\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__call__\u001B[39m,\n\u001B[1;32m   1031\u001B[0m         hidden_states,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1036\u001B[0m         use_cache,\n\u001B[1;32m   1037\u001B[0m     )\n\u001B[1;32m   1038\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1039\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m decoder_layer(\n\u001B[1;32m   1040\u001B[0m         hidden_states,\n\u001B[1;32m   1041\u001B[0m         attention_mask\u001B[38;5;241m=\u001B[39mattention_mask,\n\u001B[1;32m   1042\u001B[0m         position_ids\u001B[38;5;241m=\u001B[39mposition_ids,\n\u001B[1;32m   1043\u001B[0m         past_key_value\u001B[38;5;241m=\u001B[39mpast_key_values,\n\u001B[1;32m   1044\u001B[0m         output_attentions\u001B[38;5;241m=\u001B[39moutput_attentions,\n\u001B[1;32m   1045\u001B[0m         use_cache\u001B[38;5;241m=\u001B[39muse_cache,\n\u001B[1;32m   1046\u001B[0m     )\n\u001B[1;32m   1048\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m layer_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m   1050\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m use_cache:\n",
      "File \u001B[0;32m~/anaconda3/envs/llm/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/anaconda3/envs/llm/lib/python3.11/site-packages/torch/nn/modules/module.py:1561\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1558\u001B[0m     bw_hook \u001B[38;5;241m=\u001B[39m hooks\u001B[38;5;241m.\u001B[39mBackwardHook(\u001B[38;5;28mself\u001B[39m, full_backward_hooks, backward_pre_hooks)\n\u001B[1;32m   1559\u001B[0m     args \u001B[38;5;241m=\u001B[39m bw_hook\u001B[38;5;241m.\u001B[39msetup_input_hook(args)\n\u001B[0;32m-> 1561\u001B[0m result \u001B[38;5;241m=\u001B[39m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1562\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks:\n\u001B[1;32m   1563\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m hook_id, hook \u001B[38;5;129;01min\u001B[39;00m (\n\u001B[1;32m   1564\u001B[0m         \u001B[38;5;241m*\u001B[39m_global_forward_hooks\u001B[38;5;241m.\u001B[39mitems(),\n\u001B[1;32m   1565\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks\u001B[38;5;241m.\u001B[39mitems(),\n\u001B[1;32m   1566\u001B[0m     ):\n\u001B[1;32m   1567\u001B[0m         \u001B[38;5;66;03m# mark that always called hook is run\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/llm/lib/python3.11/site-packages/accelerate/hooks.py:166\u001B[0m, in \u001B[0;36madd_hook_to_module.<locals>.new_forward\u001B[0;34m(module, *args, **kwargs)\u001B[0m\n\u001B[1;32m    164\u001B[0m         output \u001B[38;5;241m=\u001B[39m module\u001B[38;5;241m.\u001B[39m_old_forward(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    165\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 166\u001B[0m     output \u001B[38;5;241m=\u001B[39m module\u001B[38;5;241m.\u001B[39m_old_forward(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    167\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m module\u001B[38;5;241m.\u001B[39m_hf_hook\u001B[38;5;241m.\u001B[39mpost_forward(module, output)\n",
      "File \u001B[0;32m~/anaconda3/envs/llm/lib/python3.11/site-packages/transformers/models/mistral/modeling_mistral.py:754\u001B[0m, in \u001B[0;36mMistralDecoderLayer.forward\u001B[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, **kwargs)\u001B[0m\n\u001B[1;32m    751\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minput_layernorm(hidden_states)\n\u001B[1;32m    753\u001B[0m \u001B[38;5;66;03m# Self Attention\u001B[39;00m\n\u001B[0;32m--> 754\u001B[0m hidden_states, self_attn_weights, present_key_value \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mself_attn(\n\u001B[1;32m    755\u001B[0m     hidden_states\u001B[38;5;241m=\u001B[39mhidden_states,\n\u001B[1;32m    756\u001B[0m     attention_mask\u001B[38;5;241m=\u001B[39mattention_mask,\n\u001B[1;32m    757\u001B[0m     position_ids\u001B[38;5;241m=\u001B[39mposition_ids,\n\u001B[1;32m    758\u001B[0m     past_key_value\u001B[38;5;241m=\u001B[39mpast_key_value,\n\u001B[1;32m    759\u001B[0m     output_attentions\u001B[38;5;241m=\u001B[39moutput_attentions,\n\u001B[1;32m    760\u001B[0m     use_cache\u001B[38;5;241m=\u001B[39muse_cache,\n\u001B[1;32m    761\u001B[0m )\n\u001B[1;32m    762\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m residual \u001B[38;5;241m+\u001B[39m hidden_states\n\u001B[1;32m    764\u001B[0m \u001B[38;5;66;03m# Fully Connected\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/llm/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/anaconda3/envs/llm/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/llm/lib/python3.11/site-packages/accelerate/hooks.py:166\u001B[0m, in \u001B[0;36madd_hook_to_module.<locals>.new_forward\u001B[0;34m(module, *args, **kwargs)\u001B[0m\n\u001B[1;32m    164\u001B[0m         output \u001B[38;5;241m=\u001B[39m module\u001B[38;5;241m.\u001B[39m_old_forward(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    165\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 166\u001B[0m     output \u001B[38;5;241m=\u001B[39m module\u001B[38;5;241m.\u001B[39m_old_forward(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    167\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m module\u001B[38;5;241m.\u001B[39m_hf_hook\u001B[38;5;241m.\u001B[39mpost_forward(module, output)\n",
      "File \u001B[0;32m~/anaconda3/envs/llm/lib/python3.11/site-packages/transformers/models/mistral/modeling_mistral.py:667\u001B[0m, in \u001B[0;36mMistralSdpaAttention.forward\u001B[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache)\u001B[0m\n\u001B[1;32m    665\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m past_key_value \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    666\u001B[0m     cache_kwargs \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msin\u001B[39m\u001B[38;5;124m\"\u001B[39m: sin, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcos\u001B[39m\u001B[38;5;124m\"\u001B[39m: cos}  \u001B[38;5;66;03m# Specific to RoPE models\u001B[39;00m\n\u001B[0;32m--> 667\u001B[0m     key_states, value_states \u001B[38;5;241m=\u001B[39m past_key_value\u001B[38;5;241m.\u001B[39mupdate(key_states, value_states, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayer_idx, cache_kwargs)\n\u001B[1;32m    669\u001B[0m key_states \u001B[38;5;241m=\u001B[39m repeat_kv(key_states, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_key_value_groups)\n\u001B[1;32m    670\u001B[0m value_states \u001B[38;5;241m=\u001B[39m repeat_kv(value_states, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_key_value_groups)\n",
      "File \u001B[0;32m~/anaconda3/envs/llm/lib/python3.11/site-packages/transformers/cache_utils.py:127\u001B[0m, in \u001B[0;36mDynamicCache.update\u001B[0;34m(self, key_states, value_states, layer_idx, cache_kwargs)\u001B[0m\n\u001B[1;32m    125\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvalue_cache\u001B[38;5;241m.\u001B[39mappend(value_states)\n\u001B[1;32m    126\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 127\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mkey_cache[layer_idx] \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcat([\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mkey_cache[layer_idx], key_states], dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m2\u001B[39m)\n\u001B[1;32m    128\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvalue_cache[layer_idx] \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcat([\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvalue_cache[layer_idx], value_states], dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m2\u001B[39m)\n\u001B[1;32m    130\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mkey_cache[layer_idx], \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvalue_cache[layer_idx]\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:1! (when checking argument for argument tensors in method wrapper_CUDA_cat)"
     ]
    }
   ],
   "source": [
    "# Main Experiment\n",
    "\n",
    "from core.task_vectors import run_icl, run_task_vector\n",
    "from core.data.task_helpers import get_task_by_name\n",
    "from core.analysis.evaluation import calculate_accuracy, calculate_accuracy_on_datasets, print_evaluation_summary\n",
    "\n",
    "seed_everything(41)\n",
    "\n",
    "# task_name = \"knowledge_country_capital\"\n",
    "# task_name = \"knowledge_person_language\"\n",
    "# task_name = \"knowledge_location_continent\"\n",
    "# task_name = \"knowledge_location_religion\"\n",
    "\n",
    "# task_name = \"algorithmic_prev_letter\"  # 0.57,0.40\n",
    "# task_name = \"algorithmic_next_letter\"  # 0.91, 0.94\n",
    "# task_name = \"algorithmic_list_first\"  # 1.00, 0.99\n",
    "# task_name = \"algorithmic_list_last\"  # 0.97, 0.86\n",
    "# task_name = \"algorithmic_to_upper\"  # 1.00, 0.96\n",
    "# task_name = \"algorithmic_to_lower\"  # 1.00, 0.88\n",
    "\n",
    "task_name = \"translation_fr_en\"\n",
    "# task_name = \"translation_es_en\"\n",
    "# task_name = \"translation_it_en\" \n",
    "# task_name = \"translation_en_fr\"\n",
    "# task_name = \"translation_en_es\"\n",
    "# task_name = \"translation_en_it\"\n",
    "\n",
    "# task_name = \"linguistic_present_simple_gerund\"  # 0.96, 0.80\n",
    "# task_name = \"linguistic_present_simple_past_simple\"  # 0.95, 0.94\n",
    "# task_name = \"linguistic_present_simple_past_perfect\"  # 0.79, 0.61\n",
    "# task_name = \"linguistic_plural_singular\"  # 0.90, 0.81\n",
    "# task_name = \"linguistic_antonyms\"  # 0.90, 0.88\n",
    "\n",
    "# task_name = \"sentiment\"\n",
    "\n",
    "num_examples = 5\n",
    "\n",
    "task = get_task_by_name(tokenizer, task_name)\n",
    "\n",
    "test_datasets = task.create_datasets(num_datasets=100, num_examples=num_examples)\n",
    "dev_datasets = task.create_datasets(num_datasets=50, num_examples=num_examples)\n",
    "\n",
    "icl_predictions = run_icl(model, tokenizer, task, test_datasets)\n",
    "print_evaluation_summary(task, icl_predictions, test_datasets)\n",
    "\n",
    "tv_predictions, tv_dev_accuracy_by_layer, task_hiddens = run_task_vector(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    task,\n",
    "    test_datasets,\n",
    "    dev_datasets,\n",
    "    multi_context=False\n",
    ")\n",
    "print_evaluation_summary(task, tv_predictions, test_datasets)\n",
    "# print(tv_dev_accuracy_by_layer)\n",
    "\n",
    "# icl_accuracy = calculate_accuracy_on_datasets(icl_predictions, test_datasets)\n",
    "# tv_accuracy = calculate_accuracy_on_datasets(tv_predictions, test_datasets)\n",
    "# print(f\"ICL accuracy: {icl_accuracy:.2f}\")\n",
    "# print(f\"TV accuracy: {tv_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overriding Experiment\n",
    "\n",
    "from typing import Any\n",
    "from core.task_vectors import run_icl, run_overriding_task_vector\n",
    "from core.data.task_helpers import get_task_by_name\n",
    "from core.analysis.evaluation import calculate_accuracy\n",
    "from core.data.tasks.task import Task\n",
    "from scripts.experiments.overriding import OVERRIDING_TASK_PAIRS\n",
    "\n",
    "def is_valid_input(task: Task, inp: Any) -> bool:\n",
    "    try:\n",
    "        task.calc_output(inp)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "seed_everything(41)\n",
    "\n",
    "\n",
    "task_name, overriding_task_name = OVERRIDING_TASK_PAIRS[3]\n",
    "\n",
    "num_examples = 4\n",
    "\n",
    "task = get_task_by_name(tokenizer, task_name)\n",
    "overriding_task = get_task_by_name(tokenizer, overriding_task_name)\n",
    "\n",
    "test_datasets = task.create_datasets(num_datasets=1000, num_examples=num_examples)\n",
    "overriding_datasets = overriding_task.create_datasets(num_datasets=100, num_examples=num_examples)\n",
    "\n",
    "# filter only test_datasets that are valid inputs for the overriding task\n",
    "test_datasets = [dataset for dataset in test_datasets if is_valid_input(overriding_task, dataset.test_input)]\n",
    "test_datasets = test_datasets[:len(overriding_datasets)]\n",
    "\n",
    "assert len(test_datasets) == len(overriding_datasets)\n",
    "\n",
    "icl_predictions = run_icl(model, tokenizer, task, test_datasets)\n",
    "tv_predictions, tv_dev_accuracy_by_layer, task_hiddens = run_overriding_task_vector(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    task,\n",
    "    test_datasets,\n",
    "    overriding_datasets,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "expected_outputs_original = [dataset.test_output for dataset in test_datasets]\n",
    "expected_outputs_patched = [overriding_task.calc_output(dataset.test_input) for dataset in test_datasets]\n",
    "\n",
    "icl_accuracy_original = calculate_accuracy(task,icl_predictions, expected_outputs_original)\n",
    "icl_accuracy_patched = calculate_accuracy(task,icl_predictions, expected_outputs_patched)\n",
    "\n",
    "tv_accuracy_original = calculate_accuracy(task,tv_predictions, expected_outputs_original)\n",
    "tv_accuracy_patched = calculate_accuracy(task,tv_predictions, expected_outputs_patched)\n",
    "\n",
    "print(f\"ICL accuracy original: {icl_accuracy_original:.2f}\")\n",
    "print(f\"ICL accuracy patched: {icl_accuracy_patched:.2f}\")\n",
    "print(f\"TV accuracy original: {tv_accuracy_original:.2f}\")\n",
    "print(f\"TV accuracy patched: {tv_accuracy_patched:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top tokens\n",
    "\n",
    "from core.analysis.utils import logits_top_tokens, tokens_ranks\n",
    "from core.models.utils.inference import hidden_to_logits\n",
    "from itertools import chain\n",
    "\n",
    "tv_ordered_tokens_by_layer = {}\n",
    "\n",
    "for layer_num in tv_dev_accuracy_by_layer.keys():\n",
    "    task_hidden = task_hiddens.mean(axis=0)[layer_num]\n",
    "    logits = hidden_to_logits(model, task_hidden)\n",
    "    tv_ordered_tokens_by_layer[layer_num] = logits_top_tokens(logits, tokenizer, k=500)\n",
    "    print(\"Top tokens for layer\", layer_num, \":\", tv_ordered_tokens_by_layer[layer_num][:12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if input+task=output\n",
    "\n",
    "from core.models.utils.llm_layers import get_lm_head, get_lm_pipeline\n",
    "from core.models.utils.inference import hidden_to_logits, logits_to_tokens\n",
    "\n",
    "# Find the layer that has the highest accuracy\n",
    "layer_num = max(tv_dev_accuracy_by_layer, key=tv_dev_accuracy_by_layer.get) + 3\n",
    "\n",
    "task_hidden = task_hiddens.mean(axis=0)[layer_num]\n",
    "\n",
    "embeddings = get_lm_head(model).weight.float().cpu()\n",
    "\n",
    "inputs = [dataset.test_input.strip() for dataset in test_datasets]\n",
    "\n",
    "inputs_token_ids = tokenizer(inputs, add_special_tokens=False).input_ids\n",
    "\n",
    "inputs_token_ids = [x[0] for x in inputs_token_ids if len(x) == 1]\n",
    "\n",
    "input_token_embeddings = embeddings[inputs_token_ids].cpu()\n",
    "\n",
    "# normalize embeddings and task_hidden\n",
    "input_token_embeddings = input_token_embeddings / input_token_embeddings.norm(dim=-1, keepdim=True)\n",
    "task_hidden = task_hidden / task_hidden.norm(dim=-1, keepdim=True)\n",
    "\n",
    "input_plus_task_embeddings = input_token_embeddings + task_hidden * 1.0\n",
    "\n",
    "logits = hidden_to_logits(model, input_plus_task_embeddings)\n",
    "# logits = embeddings @ input_plus_task_embeddings.T\n",
    "\n",
    "ignore_ids=inputs_token_ids\n",
    "# ignore_ids=None\n",
    "\n",
    "outputs = logits_to_tokens(logits, tokenizer, ignore_ids=ignore_ids)\n",
    "\n",
    "list(zip(inputs, outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.models.utils.inference import batch_generate, tokenize_prompts\n",
    "\n",
    "from transformers import TextGenerationPipeline\n",
    "\n",
    "prompt_examples = [\n",
    "    \"Canada -> Ottawa\",\n",
    "    # \"Australia -> Canberra\",\n",
    "    \"France -> Paris\",\n",
    "    \"Germany -> Berlin\",\n",
    "    # \"Australia -> Sydney\",\n",
    "    \"Switzerland ->\",\n",
    "    # \"India -> Mumbai\",\n",
    "    # \"China -> Shanghai\",\n",
    "    # \"Australia ->\"\n",
    "]\n",
    "\n",
    "prompt = \"\\n\".join(prompt_examples)\n",
    "\n",
    "pipeline = TextGenerationPipeline(model, tokenizer)\n",
    "\n",
    "completion = pipeline(prompt, max_new_tokens=2, num_return_sequences=1, do_sample=False)\n",
    "\n",
    "print(completion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
